{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LDL-C Model Training Workflow\n",
                "\n",
                "This notebook demonstrates the complete machine learning training workflow for the hybrid LDL-C estimation model. We train three models:\n",
                "\n",
                "1. **Ridge Regression**: Simple linear model with L2 regularization (baseline)\n",
                "2. **Random Forest**: Ensemble of decision trees for nonlinear patterns\n",
                "3. **LightGBM**: Gradient boosting for best performance\n",
                "\n",
                "All models use equation predictions (Friedewald, Martin-Hopkins, Sampson) as features, learning optimal combinations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "matplotlib.use('Agg')  # Non-interactive backend for notebook execution\n",
                "import warnings\n",
                "import sys\n",
                "import os\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from ldlC.train import (\n",
                "    create_features,\n",
                "    stratified_split,\n",
                "    train_ridge,\n",
                "    train_random_forest,\n",
                "    train_lightgbm,\n",
                "    cross_validate_model,\n",
                "    save_model\n",
                ")\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "np.random.seed(42)\n",
                "\n",
                "print('All imports successful!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Generate Synthetic Training Data\n",
                "\n",
                "Since NHANES data requires download, we generate synthetic data that mimics real lipid panel distributions. This allows the notebook to execute without external data dependencies.\n",
                "\n",
                "**Note**: For actual training, replace this cell with NHANES data loading using the `data.py` module."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic lipid panel data\n",
                "n_samples = 2000\n",
                "\n",
                "# Create realistic distributions\n",
                "tc_mgdl = np.random.normal(200, 40, n_samples).clip(100, 350)\n",
                "hdl_mgdl = np.random.normal(55, 15, n_samples).clip(25, 100)\n",
                "# Keep TG <= 400 to avoid Friedewald NaN issues in synthetic demo\n",
                "tg_mgdl = np.random.lognormal(np.log(120), 0.4, n_samples).clip(40, 400)\n",
                "\n",
                "# Calculate non-HDL\n",
                "non_hdl_mgdl = tc_mgdl - hdl_mgdl\n",
                "\n",
                "# Generate synthetic \"direct\" LDL values\n",
                "# Base LDL from modified Friedewald with realistic noise\n",
                "base_ldl = tc_mgdl - hdl_mgdl - (tg_mgdl / np.clip(5 + 0.005 * tg_mgdl, 5, 12))\n",
                "noise = np.random.normal(0, 8, n_samples)  # Measurement noise\n",
                "ldl_direct_mgdl = (base_ldl + noise).clip(30, 250)\n",
                "\n",
                "# Create DataFrame\n",
                "df = pd.DataFrame({\n",
                "    'tc_mgdl': tc_mgdl,\n",
                "    'hdl_mgdl': hdl_mgdl,\n",
                "    'tg_mgdl': tg_mgdl,\n",
                "    'non_hdl_mgdl': non_hdl_mgdl,\n",
                "    'ldl_direct_mgdl': ldl_direct_mgdl\n",
                "})\n",
                "\n",
                "print(f'Generated {len(df)} synthetic samples')\n",
                "print('\\nData summary:')\n",
                "df.describe().round(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize TG distribution\n",
                "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
                "\n",
                "# TG distribution\n",
                "axes[0].hist(df['tg_mgdl'], bins=40, edgecolor='black', alpha=0.7, color='#3498db')\n",
                "axes[0].axvline(x=150, color='orange', linestyle='--', label='TG=150 (borderline)')\n",
                "axes[0].axvline(x=400, color='red', linestyle='--', label='TG=400 (Friedewald limit)')\n",
                "axes[0].set_xlabel('Triglycerides (mg/dL)')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].set_title('TG Distribution')\n",
                "axes[0].legend(fontsize=8)\n",
                "\n",
                "# LDL distribution\n",
                "axes[1].hist(df['ldl_direct_mgdl'], bins=40, edgecolor='black', alpha=0.7, color='#2ecc71')\n",
                "axes[1].axvline(x=70, color='green', linestyle='--', label='LDL=70 (optimal)')\n",
                "axes[1].axvline(x=100, color='orange', linestyle='--', label='LDL=100 (near optimal)')\n",
                "axes[1].axvline(x=130, color='red', linestyle='--', label='LDL=130 (borderline high)')\n",
                "axes[1].set_xlabel('LDL-direct (mg/dL)')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "axes[1].set_title('LDL Distribution')\n",
                "axes[1].legend(fontsize=8)\n",
                "\n",
                "# TC vs HDL scatter\n",
                "scatter = axes[2].scatter(df['hdl_mgdl'], df['tc_mgdl'], c=df['tg_mgdl'], \n",
                "                          cmap='YlOrRd', alpha=0.5, s=10)\n",
                "axes[2].set_xlabel('HDL (mg/dL)')\n",
                "axes[2].set_ylabel('TC (mg/dL)')\n",
                "axes[2].set_title('TC vs HDL (colored by TG)')\n",
                "plt.colorbar(scatter, ax=axes[2], label='TG (mg/dL)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_data_distributions.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('Saved: training_data_distributions.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Engineering\n",
                "\n",
                "Create features using the `create_features()` function. Features include:\n",
                "- Raw lipid values: TC, HDL, TG, non-HDL\n",
                "- Ratio features: TG/HDL, TC/HDL\n",
                "- Equation predictions: Friedewald, Martin-Hopkins, Extended M-H, Sampson"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create features\n",
                "X, feature_names = create_features(df)\n",
                "\n",
                "print(f'Generated {len(feature_names)} features:')\n",
                "for i, name in enumerate(feature_names, 1):\n",
                "    print(f'  {i}. {name}')\n",
                "\n",
                "print('\\nFeature statistics:')\n",
                "X.describe().round(2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Stratified Train/Test Split\n",
                "\n",
                "Split data ensuring proportional representation of TG strata:\n",
                "- < 100 mg/dL (normal)\n",
                "- 100-150 mg/dL (borderline)\n",
                "- 150-200 mg/dL (borderline high)\n",
                "- 200-400 mg/dL (high)\n",
                "- > 400 mg/dL (very high)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform stratified split\n",
                "X_train, X_test, y_train, y_test = stratified_split(df, test_size=0.3, random_state=42)\n",
                "\n",
                "# Remove any rows with NaN (from Friedewald at TG > 400, if present)\n",
                "train_valid = ~X_train.isna().any(axis=1)\n",
                "test_valid = ~X_test.isna().any(axis=1)\n",
                "X_train = X_train[train_valid].reset_index(drop=True)\n",
                "y_train = y_train[train_valid].reset_index(drop=True)\n",
                "X_test = X_test[test_valid].reset_index(drop=True)\n",
                "y_test = y_test[test_valid].reset_index(drop=True)\n",
                "\n",
                "print(f'Training samples: {len(X_train)}')\n",
                "print(f'Test samples: {len(X_test)}')\n",
                "print(f'\\nFeatures shape: {X_train.shape[1]} columns')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify TG stratification\n",
                "def assign_tg_stratum(tg):\n",
                "    if tg < 100:\n",
                "        return '<100'\n",
                "    elif tg < 150:\n",
                "        return '100-150'\n",
                "    elif tg < 200:\n",
                "        return '150-200'\n",
                "    elif tg < 400:\n",
                "        return '200-400'\n",
                "    else:\n",
                "        return '>400'\n",
                "\n",
                "train_strata = X_train['tg_mgdl'].apply(assign_tg_stratum).value_counts(normalize=True).sort_index()\n",
                "test_strata = X_test['tg_mgdl'].apply(assign_tg_stratum).value_counts(normalize=True).sort_index()\n",
                "\n",
                "strata_comparison = pd.DataFrame({\n",
                "    'Train %': (train_strata * 100).round(1),\n",
                "    'Test %': (test_strata * 100).round(1)\n",
                "})\n",
                "\n",
                "print('TG Stratum Distribution (should be similar):')\n",
                "strata_comparison"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Training\n",
                "\n",
                "Train three models with increasing complexity:\n",
                "1. Ridge Regression (linear baseline)\n",
                "2. Random Forest (nonlinear ensemble)\n",
                "3. LightGBM (gradient boosting)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split training data for LightGBM validation\n",
                "val_size = int(len(X_train) * 0.2)\n",
                "X_val = X_train.iloc[-val_size:].reset_index(drop=True)\n",
                "y_val = y_train.iloc[-val_size:].reset_index(drop=True)\n",
                "X_train_lgb = X_train.iloc[:-val_size].reset_index(drop=True)\n",
                "y_train_lgb = y_train.iloc[:-val_size].reset_index(drop=True)\n",
                "\n",
                "print(f'LightGBM training: {len(X_train_lgb)} samples')\n",
                "print(f'LightGBM validation: {len(X_val)} samples')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Ridge Regression\n",
                "print('Training Ridge Regression...')\n",
                "ridge_model = train_ridge(X_train, y_train, alpha=1.0)\n",
                "print('  ✓ Ridge trained')\n",
                "\n",
                "# Train Random Forest\n",
                "print('Training Random Forest (200 trees)...')\n",
                "rf_model = train_random_forest(X_train, y_train, n_estimators=200)\n",
                "print('  ✓ Random Forest trained')\n",
                "\n",
                "# Train LightGBM\n",
                "print('Training LightGBM with early stopping...')\n",
                "lgb_model = train_lightgbm(X_train_lgb, y_train_lgb, X_val, y_val)\n",
                "print(f'  ✓ LightGBM trained (best iteration: {lgb_model.best_iteration_})')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Cross-Validation Comparison\n",
                "\n",
                "Evaluate models using 10-fold cross-validation to get robust performance estimates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare complete feature set for CV\n",
                "X_full, _ = create_features(df)\n",
                "y_full = df['ldl_direct_mgdl']\n",
                "\n",
                "# Drop any rows with NaN in features\n",
                "valid_idx = ~X_full.isna().any(axis=1)\n",
                "X_full = X_full[valid_idx].reset_index(drop=True)\n",
                "y_full = y_full[valid_idx].reset_index(drop=True)\n",
                "\n",
                "print(f'Samples for CV: {len(X_full)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-validate each model\n",
                "print('Running 10-fold cross-validation...')\n",
                "print('(This may take a minute for Random Forest)\\n')\n",
                "\n",
                "# Ridge CV\n",
                "ridge_cv = cross_validate_model(Ridge(alpha=1.0), X_full, y_full, n_splits=10)\n",
                "print(f'Ridge: RMSE = {ridge_cv[\"RMSE_mean\"]:.2f} ± {ridge_cv[\"RMSE_std\"]:.2f}')\n",
                "\n",
                "# Random Forest CV (fewer trees for speed)\n",
                "rf_cv = cross_validate_model(RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1), \n",
                "                             X_full, y_full, n_splits=10)\n",
                "print(f'Random Forest: RMSE = {rf_cv[\"RMSE_mean\"]:.2f} ± {rf_cv[\"RMSE_std\"]:.2f}')\n",
                "\n",
                "# Note: LightGBM CV requires special handling due to early stopping\n",
                "# We'll use the test set performance instead\n",
                "print('\\n(LightGBM evaluated on held-out test set instead of CV)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create CV results comparison table\n",
                "cv_results = pd.DataFrame({\n",
                "    'Model': ['Ridge', 'Random Forest'],\n",
                "    'RMSE Mean': [ridge_cv['RMSE_mean'], rf_cv['RMSE_mean']],\n",
                "    'RMSE Std': [ridge_cv['RMSE_std'], rf_cv['RMSE_std']],\n",
                "    'MAE Mean': [ridge_cv['MAE_mean'], rf_cv['MAE_mean']],\n",
                "    'MAE Std': [ridge_cv['MAE_std'], rf_cv['MAE_std']]\n",
                "}).round(2)\n",
                "\n",
                "print('Cross-Validation Results:')\n",
                "cv_results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test Set Performance\n",
                "\n",
                "Evaluate all three models on the held-out test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions from each model\n",
                "y_pred_ridge = ridge_model.predict(X_test)\n",
                "y_pred_rf = rf_model.predict(X_test)\n",
                "y_pred_lgb = lgb_model.predict(X_test)\n",
                "\n",
                "# Calculate metrics\n",
                "def calc_metrics(y_true, y_pred):\n",
                "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    bias = np.mean(y_pred - y_true)\n",
                "    r = np.corrcoef(y_true, y_pred)[0, 1]\n",
                "    return {'RMSE': rmse, 'MAE': mae, 'Bias': bias, 'R': r}\n",
                "\n",
                "ridge_metrics = calc_metrics(y_test, y_pred_ridge)\n",
                "rf_metrics = calc_metrics(y_test, y_pred_rf)\n",
                "lgb_metrics = calc_metrics(y_test, y_pred_lgb)\n",
                "\n",
                "# Create comparison table\n",
                "test_results = pd.DataFrame({\n",
                "    'Model': ['Ridge', 'Random Forest', 'LightGBM'],\n",
                "    'RMSE': [ridge_metrics['RMSE'], rf_metrics['RMSE'], lgb_metrics['RMSE']],\n",
                "    'MAE': [ridge_metrics['MAE'], rf_metrics['MAE'], lgb_metrics['MAE']],\n",
                "    'Bias': [ridge_metrics['Bias'], rf_metrics['Bias'], lgb_metrics['Bias']],\n",
                "    'R': [ridge_metrics['R'], rf_metrics['R'], lgb_metrics['R']]\n",
                "}).round(3)\n",
                "\n",
                "print('Test Set Performance:')\n",
                "test_results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions vs actual\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "models = [('Ridge', y_pred_ridge, '#e74c3c'),\n",
                "          ('Random Forest', y_pred_rf, '#3498db'),\n",
                "          ('LightGBM', y_pred_lgb, '#2ecc71')]\n",
                "\n",
                "for ax, (name, y_pred, color) in zip(axes, models):\n",
                "    ax.scatter(y_test, y_pred, alpha=0.3, s=10, color=color)\n",
                "    ax.plot([30, 250], [30, 250], 'k--', lw=2, label='Perfect prediction')\n",
                "    ax.set_xlabel('Actual LDL-direct (mg/dL)')\n",
                "    ax.set_ylabel('Predicted LDL (mg/dL)')\n",
                "    ax.set_title(f'{name}\\nRMSE: {calc_metrics(y_test, y_pred)[\"RMSE\"]:.2f} mg/dL')\n",
                "    ax.set_xlim(30, 250)\n",
                "    ax.set_ylim(30, 250)\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('model_predictions_scatter.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('Saved: model_predictions_scatter.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Importance\n",
                "\n",
                "Analyze which features contribute most to predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance from Random Forest and LightGBM\n",
                "rf_importance = pd.DataFrame({\n",
                "    'Feature': feature_names,\n",
                "    'RF Importance': rf_model.feature_importances_\n",
                "}).sort_values('RF Importance', ascending=False)\n",
                "\n",
                "lgb_importance = pd.DataFrame({\n",
                "    'Feature': feature_names,\n",
                "    'LGB Importance': lgb_model.feature_importances_\n",
                "}).sort_values('LGB Importance', ascending=False)\n",
                "\n",
                "# Merge and display\n",
                "importance_df = rf_importance.merge(lgb_importance, on='Feature')\n",
                "importance_df['RF Rank'] = range(1, len(importance_df) + 1)\n",
                "importance_df = importance_df.sort_values('LGB Importance', ascending=False)\n",
                "importance_df['LGB Rank'] = range(1, len(importance_df) + 1)\n",
                "\n",
                "print('Feature Importance Ranking:')\n",
                "importance_df[['Feature', 'RF Importance', 'RF Rank', 'LGB Importance', 'LGB Rank']].round(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Random Forest importance\n",
                "rf_sorted = rf_importance.sort_values('RF Importance', ascending=True)\n",
                "axes[0].barh(rf_sorted['Feature'], rf_sorted['RF Importance'], color='#3498db')\n",
                "axes[0].set_xlabel('Importance')\n",
                "axes[0].set_title('Random Forest Feature Importance', fontweight='bold')\n",
                "\n",
                "# LightGBM importance\n",
                "lgb_sorted = lgb_importance.sort_values('LGB Importance', ascending=True)\n",
                "axes[1].barh(lgb_sorted['Feature'], lgb_sorted['LGB Importance'], color='#2ecc71')\n",
                "axes[1].set_xlabel('Importance')\n",
                "axes[1].set_title('LightGBM Feature Importance', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('Saved: feature_importance.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Best Model\n",
                "\n",
                "Save the best-performing model to the `models/` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create models directory if needed\n",
                "models_dir = os.path.join('..', 'models')\n",
                "os.makedirs(models_dir, exist_ok=True)\n",
                "\n",
                "# Determine best model based on test RMSE\n",
                "model_rmses = {\n",
                "    'ridge': ridge_metrics['RMSE'],\n",
                "    'random_forest': rf_metrics['RMSE'],\n",
                "    'lightgbm': lgb_metrics['RMSE']\n",
                "}\n",
                "best_model_name = min(model_rmses, key=model_rmses.get)\n",
                "best_model = {'ridge': ridge_model, 'random_forest': rf_model, 'lightgbm': lgb_model}[best_model_name]\n",
                "\n",
                "print(f'Best model: {best_model_name.upper()} (RMSE: {model_rmses[best_model_name]:.3f} mg/dL)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all models\n",
                "save_model(ridge_model, os.path.join(models_dir, 'ridge_model.joblib'))\n",
                "save_model(rf_model, os.path.join(models_dir, 'random_forest_model.joblib'))\n",
                "save_model(lgb_model, os.path.join(models_dir, 'lightgbm_model.joblib'))\n",
                "\n",
                "# Save best model with a special name\n",
                "save_model(best_model, os.path.join(models_dir, 'best_model.joblib'))\n",
                "\n",
                "print('Models saved to ../models/:')\n",
                "print('  - ridge_model.joblib')\n",
                "print('  - random_forest_model.joblib')\n",
                "print('  - lightgbm_model.joblib')\n",
                "print(f'  - best_model.joblib ({best_model_name})')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **All models beat simple equation predictions** by learning optimal feature combinations\n",
                "2. **Equation predictions are important features**, confirming the hybrid approach value\n",
                "3. **LightGBM typically performs best** due to gradient boosting's ability to capture complex interactions\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Phase 4: Comprehensive evaluation with Bland-Altman analysis and TG-stratified metrics\n",
                "- Validate on real NHANES data with beta-quantification reference\n",
                "- Compare hybrid model to individual equations across TG strata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Notebook completed successfully!')\n",
                "print('\\nGenerated files:')\n",
                "print('  - training_data_distributions.png')\n",
                "print('  - model_predictions_scatter.png')\n",
                "print('  - feature_importance.png')\n",
                "print('\\nSaved models:')\n",
                "print('  - ../models/ridge_model.joblib')\n",
                "print('  - ../models/random_forest_model.joblib')\n",
                "print('  - ../models/lightgbm_model.joblib')\n",
                "print('  - ../models/best_model.joblib')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}