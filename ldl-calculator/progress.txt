# Progress Log

## Learnings
(Patterns discovered during implementation)

---

## Iteration 1 - US-001: Create Project Structure and Dependencies
- What was implemented:
  - Created `ldlC/` package directory with `__init__.py`, `models.py`, `utils.py`, `data.py`
  - Created `tests/` directory with `__init__.py` and `test_models.py`
  - Created `notebooks/` directory with `.gitkeep`
  - Created `requirements.txt` with all required dependencies
- Files changed:
  - `ldlC/__init__.py` (new)
  - `ldlC/models.py` (new)
  - `ldlC/utils.py` (new)
  - `ldlC/data.py` (new)
  - `tests/__init__.py` (new)
  - `tests/test_models.py` (new)
  - `notebooks/.gitkeep` (new)
  - `requirements.txt` (new)
- Learnings for future iterations:
  - All modules have docstrings for future implementations
  - pytest import already in test_models.py for future tests
---

## Iteration 2 - US-002: Implement Unit Conversion Utilities
- What was implemented:
  - Added `mg_dl_to_mmol_l(value, molecule='cholesterol')` function in `ldlC/utils.py`
  - Added `mmol_l_to_mg_dl(value, molecule='cholesterol')` function
  - Supports conversions for cholesterol (÷38.67) and triglycerides (÷88.57)
  - Created comprehensive unit tests in `tests/test_utils.py`
- Files changed:
  - `ldlC/utils.py` (modified - added conversion functions)
  - `tests/test_utils.py` (new - 18 test cases)
- Learnings for future iterations:
  - Conversion factors: Cholesterol = 38.67, Triglycerides = 88.57
  - Molecule type is case-insensitive and supports aliases (tg, triglyceride, triglycerides)
  - Use `python -m pytest` instead of `pytest` on Windows if not in PATH
---

## Iteration 3 - US-003: Create NHANES Lipid Download Module
- What was implemented:
  - Added `download_nhanes_lipids(output_dir, cycles)` function in `ldlC/data.py`
  - Downloads TRIGLY, HDL, TCHOL XPT files for cycles 2005-2018
  - Downloads BIOPRO files (contains direct LDL measurements)
  - Creates `data/raw/` directory if not exists
  - Handles HTTP errors, URL errors gracefully with informative messages
  - Helper function `_download_file` for reusable download logic
- Files changed:
  - `ldlC/data.py` (modified - added download functions)
- Learnings for future iterations:
  - NHANES cycle suffixes: D=2005-06, E=2007-08, F=2009-10, G=2011-12, H=2013-14, I=2015-16, J=2017-18
  - Standard lipid files: TRIGLY, HDL, TCHOL (+ cycle suffix)
  - Direct LDL is in BIOPRO biochemistry profile files
  - PowerShell doesn't support `&&` for command chaining - use separate commands
---

## Iteration 4 - US-004: Implement XPT File Parser
- What was implemented:
  - Added `read_xpt(filepath)` function in `ldlC/data.py`
  - Uses pandas `read_sas()` with format='xport' to parse SAS transport files
  - Returns pandas DataFrame with all data from the XPT file
  - Raises FileNotFoundError with informative message if file doesn't exist
  - Raises ValueError with helpful message if file can't be parsed
  - Added pandas import to data.py
  - Created comprehensive unit tests in `tests/test_data.py`
- Files changed:
  - `ldlC/data.py` (modified - added read_xpt function and pandas import)
  - `tests/test_data.py` (new - 4 test cases)
- Learnings for future iterations:
  - pandas can read XPT files directly with `pd.read_sas(filepath, format='xport')`
  - pyreadstat module can be used to create test XPT files if needed
  - Test file for data module is separate from models tests
---

## Iteration 5 - US-005: Create NHANES Lipid Data Cleaning Pipeline
- What was implemented:
  - Added `clean_lipid_data(tc_df, hdl_df, tg_df, ldl_direct_df)` function in `ldlC/data.py`
  - Function merges TC, HDL, TG, and direct LDL DataFrames on SEQN (sample ID)
  - Renames columns to standardized names: tc_mgdl, hdl_mgdl, tg_mgdl, ldl_direct_mgdl
  - Removes physiologic outliers: TC < 50, TG > 2000, HDL < 10 mg/dL
  - Calculates non_hdl_mgdl (TC - HDL)
  - Supports custom column names via optional parameters
  - Drops rows with missing values before cleaning
- Files changed:
  - `ldlC/data.py` (modified - added clean_lipid_data function)
- Learnings for future iterations:
  - NHANES column names: LBXTC (TC), LBDHDD (HDL), LBXSTR (TG), LBDLDL (direct LDL)
  - Inner join ensures only complete records with all lipid measurements are kept
  - Outlier thresholds based on physiologically implausible values
---

## Iteration 6 - US-006: Generate Data Quality Report
- What was implemented:
  - Added `generate_quality_report(df, output_path)` function in `ldlC/data.py`
  - Report includes: record count, mean/SD/min/max for TC/HDL/TG/LDL-direct
  - Includes missing value counts for each lipid column
  - Includes TG distribution breakdown (<150, 150-400, 400-800, >800 mg/dL) with percentages
  - Saves formatted report to specified path as text file
  - Returns dict with all metrics for programmatic access
  - Creates parent directories if they don't exist
- Files changed:
  - `ldlC/data.py` (modified - added generate_quality_report function)
- Learnings for future iterations:
  - TG distribution thresholds based on clinical guidelines for hypertriglyceridemia
  - Function returns both a text file and dict for flexibility
  - All 22 existing tests continue to pass
---

## Iteration 7 - US-007: Create Data Sourcing Notebook
- What was implemented:
  - Created `notebooks/01_data_sourcing.ipynb` with full data pipeline documentation
  - Notebook demonstrates: downloading, parsing, cleaning, and quality report generation
  - Includes markdown documentation for each step with NHANES column name reference table
  - Visualizes TG distribution histogram with clinical thresholds
  - Visualizes TG distribution bar chart by clinical category
  - Visualizes LDL distribution with ATP III guideline thresholds
  - Uses synthetic demo data so notebook executes without requiring actual NHANES downloads
- Files changed:
  - `notebooks/01_data_sourcing.ipynb` (new)
  - `PRD.md` (modified - marked US-007 complete)
- Learnings for future iterations:
  - Jupyter may not be installed - verify notebook code via Python script execution
  - Use `matplotlib.use('Agg')` for non-interactive testing
  - Synthetic demo data allows notebook to execute without external data dependencies
---

## Iteration 8 - US-009: Test Friedewald against published values
- What was implemented:
  - Added comprehensive test suite for `calc_ldl_friedewald()` in `tests/test_models.py`
  - Test case: TC=200, HDL=50, TG=150 → LDL = 120 mg/dL (verified)
  - Test case: TC=180, HDL=45, TG=100 → LDL = 115 mg/dL (verified)
  - Tests for high TG warning/NaN behavior (TG > 400 returns NaN with warning)
  - Tests for invalid inputs: negative values, NaN, None all raise ValueError
  - Additional edge case tests: TG exactly 400, TG=0, integer inputs
- Files changed:
  - `tests/test_models.py` (modified - added 12 Friedewald tests)
- Learnings for future iterations:
  - Use python3 instead of python on macOS
  - Friedewald formula: LDL = TC - HDL - (TG / 5)
  - All 12 Friedewald tests pass
---

## Iteration 9 - US-010: Implement Martin-Hopkins Equation
- What was implemented:
  - Added `calc_ldl_martin_hopkins(tc_mgdl, hdl_mgdl, tg_mgdl)` function in `ldlC/models.py`
  - Implemented 180-cell lookup table for TG:VLDL adjustment factor
  - Table has 30 rows (Non-HDL-C ranges from 7-13975 mg/dL) and 6 columns (TG ranges)
  - Factors range from 3.1 to 11.9 depending on non-HDL and TG levels
  - Works for TG up to 800 mg/dL (vs Friedewald which fails above 400)
  - Added 11 unit tests including comparison tests with Friedewald
- Files changed:
  - `ldlC/models.py` (modified - added Martin-Hopkins function and lookup table)
  - `tests/test_models.py` (modified - added 11 Martin-Hopkins tests)
  - `PRD.md` (modified - marked US-010 complete)
- Learnings for future iterations:
  - Martin-Hopkins factors: range from 3.1 (low non-HDL, high TG) to 11.9 (high non-HDL, low TG)
  - When TG=0, both Friedewald and Martin-Hopkins give same result (TC - HDL)
  - Martin-Hopkins continues to work beyond TG 400 where Friedewald fails
  - All 23 model tests now pass
---

## Iteration 10 - US-011: Implement Sampson (NIH Equation 2)
- What was implemented:
  - Verified existing `calc_ldl_sampson(tc_mgdl, hdl_mgdl, tg_mgdl)` function in `ldlC/models.py`
  - Full Sampson formula: LDL = TC/0.948 - HDL/0.971 - (TG/8.56 + TG*non-HDL/2140 - TG²/16100) - 9.44
  - Works for TG up to 800 mg/dL (vs Friedewald which fails above 400)
  - Added 12 comprehensive unit tests including validation of output in valid range (0 < LDL < TC)
- Files changed:
  - `tests/test_models.py` (modified - added 12 Sampson tests)
  - `PRD.md` (modified - marked US-011 complete)
- Learnings for future iterations:
  - Sampson equation includes quadratic TG term (TG²/16100) which helps for high TG
  - Unlike Friedewald/Martin-Hopkins, Sampson doesn't use a divisor factor but direct coefficients
  - All 34 model tests now pass
---

## Iteration 11 - US-012: Implement Extended Martin-Hopkins for Very High TG
- What was implemented:
  - Verified existing `calc_ldl_martin_hopkins_extended(tc_mgdl, hdl_mgdl, tg_mgdl)` function in `ldlC/models.py`
  - Uses extended coefficient table with 10 TG ranges for finer granularity at high TG (400-800 mg/dL)
  - Extended table has same 30 non-HDL-C rows but 10 TG columns instead of 6
  - Factors decrease as TG increases (e.g., for high non-HDL: 11.9 → 6.0 from TG <100 to TG ≥800)
  - 11 comprehensive unit tests for high-TG cases already exist
- Files changed:
  - `PRD.md` (modified - marked US-012 complete)
- Learnings for future iterations:
  - Extended M-H differs from standard M-H primarily at TG > 400 where it has finer granularity
  - All 45 model tests now pass
---

## Iteration 12 - US-013: Create Equation Comparison Notebook
- What was implemented:
  - Created `notebooks/02_equation_comparison.ipynb` with comprehensive equation comparison
  - Generates synthetic grid of TC (150-300), HDL (40-70), TG (50-800) combinations
  - Creates heatmaps showing differences between equations across TG and TC ranges
  - Compares all four equations: Friedewald, Martin-Hopkins, Extended M-H, Sampson
  - Includes line plots showing LDL estimates as TG increases for fixed TC/HDL
  - Includes detailed markdown interpretation of when each equation excels
  - Generates visual recommendation matrix by TG stratum
- Files changed:
  - `notebooks/02_equation_comparison.ipynb` (new)
  - `PRD.md` (modified - marked US-013 complete)
- Learnings for future iterations:
  - Friedewald is unreliable for TG > 400, returns NaN
  - Martin-Hopkins and Sampson differ most at high TG (400-800 mg/dL)
  - Extended M-H provides finer granularity than standard M-H at high TG
  - All 66 tests pass
---

## Iteration 13 - US-014: Create Feature Engineering Module
- What was implemented:
  - Created `ldlC/train.py` module for ML training functions
  - Added `create_features(df)` function that generates feature matrix for ML training
  - Features include raw lipid values: tc_mgdl, hdl_mgdl, tg_mgdl, non_hdl_mgdl
  - Ratio features: tg_hdl_ratio, tc_hdl_ratio
  - Baseline equation predictions: ldl_friedewald, ldl_martin_hopkins, ldl_martin_hopkins_extended, ldl_sampson
  - Function returns (X, feature_names) tuple for use in training
  - Handles invalid inputs gracefully with NaN values
- Files changed:
  - `ldlC/train.py` (new)
  - `PRD.md` (modified - marked US-013 and US-014 complete)
- Learnings for future iterations:
  - Equation predictions are wrapped in try/except for robustness
  - Feature matrix uses pandas apply() for row-wise equation calculations
  - All 66 tests continue to pass
---

## Iteration 14 - US-015: Implement Train/Test Split with Stratification
- What was implemented:
  - Added `stratified_split(df, test_size=0.3, random_state=42)` function in `ldlC/train.py`
  - Stratifies by TG strata: < 100, 100-150, 150-200, 200-400, > 400 mg/dL
  - Uses sklearn's StratifiedShuffleSplit to ensure proportional representation
  - Returns (X_train, X_test, y_train, y_test) tuple
  - Validates required columns and minimum data size
  - Integrates with create_features() for feature generation
- Files changed:
  - `ldlC/train.py` (modified - added stratified_split function)
  - `PRD.md` (modified - marked US-015 complete)
- Learnings for future iterations:
  - TG strata based on clinical guidelines for hypertriglyceridemia
  - StratifiedShuffleSplit preserves class proportions in train/test splits
  - All 66 tests continue to pass
---

## Iteration 15 - US-017: Train Random Forest Model
- What was implemented:
  - Added `train_random_forest(X_train, y_train, n_estimators=200)` function in `ldlC/train.py`
  - Function trains a RandomForestRegressor with configurable n_estimators parameter
  - Uses random_state=42 for reproducibility and n_jobs=-1 for parallel processing
  - Validates input data shapes (X_train and y_train must have same length)
  - Returns fitted RandomForestRegressor model
- Files changed:
  - `ldlC/train.py` (modified - added RandomForestRegressor import and train_random_forest function)
  - `PRD.md` (modified - marked US-017 complete)
- Learnings for future iterations:
  - RandomForestRegressor from sklearn.ensemble for ensemble learning
  - Pattern follows same structure as train_ridge() for consistency
  - All 66 tests continue to pass
---

## Iteration 16 - US-018: Train LightGBM Model
- What was implemented:
  - Added `train_lightgbm(X_train, y_train, X_val, y_val)` function in `ldlC/train.py`
  - Uses LGBMRegressor from lightgbm package
  - Implements early stopping with 20 rounds (configurable)
  - Uses validation set for early stopping to prevent overfitting
  - Default n_estimators=1000 (early stopping typically terminates before this)
  - Uses random_state=42 for reproducibility and n_jobs=-1 for parallel processing
- Files changed:
  - `ldlC/train.py` (modified - added LGBMRegressor import and train_lightgbm function)
  - `PRD.md` (modified - marked US-018 complete)
- Learnings for future iterations:
  - LightGBM uses callbacks for early stopping (lightgbm.early_stopping())
  - verbosity=-1 suppresses training warnings in LightGBM
  - PowerShell doesn't support && for command chaining - use separate commands
  - All 66 tests continue to pass
---

## Iteration 17 - US-019: Implement Cross-Validation Wrapper
- What was implemented:
  - Added `cross_validate_model(model, X, y, n_splits=10)` function in `ldlC/train.py`
  - Uses sklearn's KFold for k-fold cross-validation with shuffling and random_state=42
  - Clones the model for each fold using sklearn.base.clone for independent training
  - Returns dict with RMSE_mean, RMSE_std, MAE_mean, MAE_std
  - Validates input data shapes and checks for minimum sample count
- Files changed:
  - `ldlC/train.py` (modified - added cross_validate_model function)
  - `PRD.md` (modified - marked US-019 complete)
- Learnings for future iterations:
  - Use sklearn.base.clone to get fresh model instance for each CV fold
  - KFold with shuffle=True and random_state=42 ensures reproducibility
  - All 66 tests continue to pass
---

## Iteration 18 - US-020: Create Model Training Notebook
- What was implemented:
  - Created `notebooks/03_model_training.ipynb` with complete ML training workflow
  - Generates synthetic data (TG capped at 400 to avoid Friedewald NaN)
  - Creates features using `create_features()` with NaN filtering
  - Performs stratified split via `stratified_split()`
  - Trains Ridge, Random Forest (200 trees), and LightGBM with early stopping
  - Shows 10-fold CV comparison table for Ridge and RF
  - Evaluates all 3 models on held-out test set with RMSE/MAE/Bias/R metrics
  - Analyzes feature importance for RF and LightGBM
  - Saves all models to `models/` directory
  - Uses synthetic demo data so notebook executes without NHANES download
- Files changed:
  - `notebooks/03_model_training.ipynb` (new)
  - `PRD.md` (modified - marked US-020 complete)
- Learnings for future iterations:
  - Friedewald returns NaN for TG > 400, must filter or cap TG in synthetic data
  - Always filter NaN after stratified_split when using equation features
  - Cannot edit .ipynb files directly, must rewrite entire notebook
  - All 66 tests continue to pass
---

## Iteration 19 - US-021: Implement Bland-Altman Analysis
- What was implemented:
  - Created `ldlC/evaluate.py` module for evaluation metrics
  - Added `bland_altman_stats(y_true, y_pred)` function for agreement analysis
  - Returns dict with: mean_bias, std_diff, loa_lower, loa_upper
  - Properly handles NaN values by excluding invalid pairs
  - Uses sample standard deviation (ddof=1) per Bland-Altman convention
  - LOA calculated as mean ± 1.96 * std (95% CI)
  - Created 12 comprehensive unit tests in `tests/test_evaluate.py`
- Files changed:
  - `ldlC/evaluate.py` (new)
  - `tests/test_evaluate.py` (new)
  - `PRD.md` (modified - marked US-021 complete)
- Learnings for future iterations:
  - Bland-Altman bias = predicted - true (sign convention)
  - Use ddof=1 for sample std in Bland-Altman analysis
  - All 77 tests now pass (11 new tests added)
---

## Iteration 20 - US-022: Implement Lin's CCC
- What was implemented:
  - Added `lins_ccc(y_true, y_pred)` function in `ldlC/evaluate.py`
  - Lin's CCC measures agreement considering both precision and accuracy
  - Formula: CCC = 2 * cov(x,y) / (var_x + var_y + (mean_x - mean_y)^2)
  - Returns CCC value between -1 (inverse agreement) and 1 (perfect agreement)
  - Properly handles NaN values by excluding invalid pairs
  - Requires at least 2 data points (unlike Bland-Altman which works with 1)
  - Added 12 comprehensive unit tests in `tests/test_evaluate.py`
- Files changed:
  - `ldlC/evaluate.py` (modified - added lins_ccc function)
  - `tests/test_evaluate.py` (modified - added 12 CCC tests)
  - `PRD.md` (modified - marked US-022 complete)
- Learnings for future iterations:
  - Lin's CCC = 2*cov / (var1 + var2 + (mean1 - mean2)^2)
  - CCC penalizes both variance differences AND mean shifts (bias)
  - Use ddof=1 for sample statistics in CCC calculation (matches Bland-Altman)
  - All 88 tests now pass (11 new tests added)
---

## Iteration 21 - US-023: Create Comprehensive Evaluation Function
- What was implemented:
  - Added `evaluate_model(y_true, y_pred, model_name)` function in `ldlC/evaluate.py`
  - Returns comprehensive dict with: model_name, n_samples, rmse, mae, bias, r_pearson, lin_ccc, ba_stats
  - Reuses existing `lins_ccc()` and `bland_altman_stats()` functions for efficiency
  - Handles NaN values by excluding invalid pairs before calculation
  - Validates inputs (requires at least 2 data points)
- Files changed:
  - `ldlC/evaluate.py` (modified - added evaluate_model function, ~100 lines)
  - `PRD.md` (modified - marked US-023 complete)
- Learnings for future iterations:
  - Pearson r is computed via `np.corrcoef()[0, 1]`
  - Existing evaluation functions can be reused inside evaluate_model
  - All 88 tests continue to pass
---

## Iteration 22 - US-024: Implement TG-Stratified Evaluation
- What was implemented:
  - Added `evaluate_by_tg_strata(y_true, y_pred, tg_values)` function in `ldlC/evaluate.py`
  - Stratifies evaluation by clinical TG thresholds: <150 (low), 150-400 (medium), 400-800 (high) mg/dL
  - Returns dict with 'low_tg', 'medium_tg', 'high_tg', 'overall' keys
  - Each stratum contains full metrics from evaluate_model() or None if insufficient samples
  - Handles NaN values and validates input array lengths
  - Reuses existing evaluate_model() function for consistent metrics
- Files changed:
  - `ldlC/evaluate.py` (modified - added evaluate_by_tg_strata function, ~120 lines)
  - `PRD.md` (modified - marked US-024 complete)
- Learnings for future iterations:
  - TG strata thresholds based on clinical hypertriglyceridemia guidelines
  - Need at least 2 samples per stratum for meaningful evaluation
  - All 88 tests continue to pass
---

## Iteration 23 - US-025: Implement Bootstrap Confidence Intervals
- What was implemented:
  - Added `bootstrap_ci(y_true, y_pred, metric_func, n_bootstrap=2000)` function in `ldlC/evaluate.py`
  - Uses resampling with replacement to estimate sampling distribution of any metric
  - Returns (lower, upper, mean) tuple for reporting uncertainty
  - Uses percentile method for CI calculation (2.5th and 97.5th percentiles for 95% CI)
  - Configurable confidence_level and random_state for reproducibility
  - Handles NaN values and failed bootstrap samples gracefully
- Files changed:
  - `ldlC/evaluate.py` (modified - added bootstrap_ci function, ~115 lines)
  - `PRD.md` (modified - marked US-025 complete)
- Learnings for future iterations:
  - Use numpy's default_rng for modern random number generation
  - Percentile method is simple and robust for bootstrap CIs
  - Try/except in bootstrap loop handles edge cases like constant arrays
  - All 88 tests continue to pass
---

## Iteration 24 - US-026: Create Evaluation Notebook
- What was implemented:
  - Created `notebooks/04_evaluation.ipynb` with comprehensive model evaluation workflow
  - Evaluates all equations (Friedewald, Martin-Hopkins, Extended M-H, Sampson) and hybrid ML model
  - Generates Bland-Altman plots for all methods with LOA statistics
  - Shows TG-stratified evaluation (< 150, 150-400, 400-800 mg/dL)
  - Computes bootstrap confidence intervals for key metrics (RMSE, CCC)
  - Creates publication-ready comparison visualizations
  - Uses synthetic data so notebook executes without NHANES download
- Files changed:
  - `notebooks/04_evaluation.ipynb` (new)
  - `PRD.md` (modified - marked US-026 complete)
- Learnings for future iterations:
  - Bland-Altman plots require mean of true/pred on x-axis, difference on y-axis
  - Use wrapper functions for safe equation application (returns NaN on error)
  - TG-stratified analysis confirms Friedewald limitations at TG > 400
  - All 88 tests continue to pass
---

## Iteration 25 - Reconciliation: Mark US-025 Complete in PRD
- What was implemented:
  - Verified `bootstrap_ci` function already exists in `ldlC/evaluate.py` (from Iteration 23)
  - Confirmed all 88 tests pass and typecheck passes
  - Marked US-025 acceptance criteria as complete in PRD.md (was missed in previous iteration)
- Files changed:
  - `PRD.md` (modified - marked US-025 complete)
- Learnings for future iterations:
  - Always verify PRD.md is updated when completing a task
  - PRD and progress.txt should remain in sync
---
