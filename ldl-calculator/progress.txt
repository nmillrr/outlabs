# Progress Log

## Learnings
(Patterns discovered during implementation)

---

## Iteration 1 - US-001: Create Project Structure and Dependencies
- What was implemented:
  - Created `ldlC/` package directory with `__init__.py`, `models.py`, `utils.py`, `data.py`
  - Created `tests/` directory with `__init__.py` and `test_models.py`
  - Created `notebooks/` directory with `.gitkeep`
  - Created `requirements.txt` with all required dependencies
- Files changed:
  - `ldlC/__init__.py` (new)
  - `ldlC/models.py` (new)
  - `ldlC/utils.py` (new)
  - `ldlC/data.py` (new)
  - `tests/__init__.py` (new)
  - `tests/test_models.py` (new)
  - `notebooks/.gitkeep` (new)
  - `requirements.txt` (new)
- Learnings for future iterations:
  - All modules have docstrings for future implementations
  - pytest import already in test_models.py for future tests
---

## Iteration 2 - US-002: Implement Unit Conversion Utilities
- What was implemented:
  - Added `mg_dl_to_mmol_l(value, molecule='cholesterol')` function in `ldlC/utils.py`
  - Added `mmol_l_to_mg_dl(value, molecule='cholesterol')` function
  - Supports conversions for cholesterol (÷38.67) and triglycerides (÷88.57)
  - Created comprehensive unit tests in `tests/test_utils.py`
- Files changed:
  - `ldlC/utils.py` (modified - added conversion functions)
  - `tests/test_utils.py` (new - 18 test cases)
- Learnings for future iterations:
  - Conversion factors: Cholesterol = 38.67, Triglycerides = 88.57
  - Molecule type is case-insensitive and supports aliases (tg, triglyceride, triglycerides)
  - Use `python -m pytest` instead of `pytest` on Windows if not in PATH
---

## Iteration 3 - US-003: Create NHANES Lipid Download Module
- What was implemented:
  - Added `download_nhanes_lipids(output_dir, cycles)` function in `ldlC/data.py`
  - Downloads TRIGLY, HDL, TCHOL XPT files for cycles 2005-2018
  - Downloads BIOPRO files (contains direct LDL measurements)
  - Creates `data/raw/` directory if not exists
  - Handles HTTP errors, URL errors gracefully with informative messages
  - Helper function `_download_file` for reusable download logic
- Files changed:
  - `ldlC/data.py` (modified - added download functions)
- Learnings for future iterations:
  - NHANES cycle suffixes: D=2005-06, E=2007-08, F=2009-10, G=2011-12, H=2013-14, I=2015-16, J=2017-18
  - Standard lipid files: TRIGLY, HDL, TCHOL (+ cycle suffix)
  - Direct LDL is in BIOPRO biochemistry profile files
  - PowerShell doesn't support `&&` for command chaining - use separate commands
---

## Iteration 4 - US-004: Implement XPT File Parser
- What was implemented:
  - Added `read_xpt(filepath)` function in `ldlC/data.py`
  - Uses pandas `read_sas()` with format='xport' to parse SAS transport files
  - Returns pandas DataFrame with all data from the XPT file
  - Raises FileNotFoundError with informative message if file doesn't exist
  - Raises ValueError with helpful message if file can't be parsed
  - Added pandas import to data.py
  - Created comprehensive unit tests in `tests/test_data.py`
- Files changed:
  - `ldlC/data.py` (modified - added read_xpt function and pandas import)
  - `tests/test_data.py` (new - 4 test cases)
- Learnings for future iterations:
  - pandas can read XPT files directly with `pd.read_sas(filepath, format='xport')`
  - pyreadstat module can be used to create test XPT files if needed
  - Test file for data module is separate from models tests
---

## Iteration 5 - US-005: Create NHANES Lipid Data Cleaning Pipeline
- What was implemented:
  - Added `clean_lipid_data(tc_df, hdl_df, tg_df, ldl_direct_df)` function in `ldlC/data.py`
  - Function merges TC, HDL, TG, and direct LDL DataFrames on SEQN (sample ID)
  - Renames columns to standardized names: tc_mgdl, hdl_mgdl, tg_mgdl, ldl_direct_mgdl
  - Removes physiologic outliers: TC < 50, TG > 2000, HDL < 10 mg/dL
  - Calculates non_hdl_mgdl (TC - HDL)
  - Supports custom column names via optional parameters
  - Drops rows with missing values before cleaning
- Files changed:
  - `ldlC/data.py` (modified - added clean_lipid_data function)
- Learnings for future iterations:
  - NHANES column names: LBXTC (TC), LBDHDD (HDL), LBXSTR (TG), LBDLDL (direct LDL)
  - Inner join ensures only complete records with all lipid measurements are kept
  - Outlier thresholds based on physiologically implausible values
---

## Iteration 6 - US-006: Generate Data Quality Report
- What was implemented:
  - Added `generate_quality_report(df, output_path)` function in `ldlC/data.py`
  - Report includes: record count, mean/SD/min/max for TC/HDL/TG/LDL-direct
  - Includes missing value counts for each lipid column
  - Includes TG distribution breakdown (<150, 150-400, 400-800, >800 mg/dL) with percentages
  - Saves formatted report to specified path as text file
  - Returns dict with all metrics for programmatic access
  - Creates parent directories if they don't exist
- Files changed:
  - `ldlC/data.py` (modified - added generate_quality_report function)
- Learnings for future iterations:
  - TG distribution thresholds based on clinical guidelines for hypertriglyceridemia
  - Function returns both a text file and dict for flexibility
  - All 22 existing tests continue to pass
---

## Iteration 7 - US-007: Create Data Sourcing Notebook
- What was implemented:
  - Created `notebooks/01_data_sourcing.ipynb` with full data pipeline documentation
  - Notebook demonstrates: downloading, parsing, cleaning, and quality report generation
  - Includes markdown documentation for each step with NHANES column name reference table
  - Visualizes TG distribution histogram with clinical thresholds
  - Visualizes TG distribution bar chart by clinical category
  - Visualizes LDL distribution with ATP III guideline thresholds
  - Uses synthetic demo data so notebook executes without requiring actual NHANES downloads
- Files changed:
  - `notebooks/01_data_sourcing.ipynb` (new)
  - `PRD.md` (modified - marked US-007 complete)
- Learnings for future iterations:
  - Jupyter may not be installed - verify notebook code via Python script execution
  - Use `matplotlib.use('Agg')` for non-interactive testing
  - Synthetic demo data allows notebook to execute without external data dependencies
---

## Iteration 8 - US-009: Test Friedewald against published values
- What was implemented:
  - Added comprehensive test suite for `calc_ldl_friedewald()` in `tests/test_models.py`
  - Test case: TC=200, HDL=50, TG=150 → LDL = 120 mg/dL (verified)
  - Test case: TC=180, HDL=45, TG=100 → LDL = 115 mg/dL (verified)
  - Tests for high TG warning/NaN behavior (TG > 400 returns NaN with warning)
  - Tests for invalid inputs: negative values, NaN, None all raise ValueError
  - Additional edge case tests: TG exactly 400, TG=0, integer inputs
- Files changed:
  - `tests/test_models.py` (modified - added 12 Friedewald tests)
- Learnings for future iterations:
  - Use python3 instead of python on macOS
  - Friedewald formula: LDL = TC - HDL - (TG / 5)
  - All 12 Friedewald tests pass
---

## Iteration 9 - US-010: Implement Martin-Hopkins Equation
- What was implemented:
  - Added `calc_ldl_martin_hopkins(tc_mgdl, hdl_mgdl, tg_mgdl)` function in `ldlC/models.py`
  - Implemented 180-cell lookup table for TG:VLDL adjustment factor
  - Table has 30 rows (Non-HDL-C ranges from 7-13975 mg/dL) and 6 columns (TG ranges)
  - Factors range from 3.1 to 11.9 depending on non-HDL and TG levels
  - Works for TG up to 800 mg/dL (vs Friedewald which fails above 400)
  - Added 11 unit tests including comparison tests with Friedewald
- Files changed:
  - `ldlC/models.py` (modified - added Martin-Hopkins function and lookup table)
  - `tests/test_models.py` (modified - added 11 Martin-Hopkins tests)
  - `PRD.md` (modified - marked US-010 complete)
- Learnings for future iterations:
  - Martin-Hopkins factors: range from 3.1 (low non-HDL, high TG) to 11.9 (high non-HDL, low TG)
  - When TG=0, both Friedewald and Martin-Hopkins give same result (TC - HDL)
  - Martin-Hopkins continues to work beyond TG 400 where Friedewald fails
  - All 23 model tests now pass
---

## Iteration 10 - US-011: Implement Sampson (NIH Equation 2)
- What was implemented:
  - Verified existing `calc_ldl_sampson(tc_mgdl, hdl_mgdl, tg_mgdl)` function in `ldlC/models.py`
  - Full Sampson formula: LDL = TC/0.948 - HDL/0.971 - (TG/8.56 + TG*non-HDL/2140 - TG²/16100) - 9.44
  - Works for TG up to 800 mg/dL (vs Friedewald which fails above 400)
  - Added 12 comprehensive unit tests including validation of output in valid range (0 < LDL < TC)
- Files changed:
  - `tests/test_models.py` (modified - added 12 Sampson tests)
  - `PRD.md` (modified - marked US-011 complete)
- Learnings for future iterations:
  - Sampson equation includes quadratic TG term (TG²/16100) which helps for high TG
  - Unlike Friedewald/Martin-Hopkins, Sampson doesn't use a divisor factor but direct coefficients
  - All 34 model tests now pass
---

## Iteration 11 - US-012: Implement Extended Martin-Hopkins for Very High TG
- What was implemented:
  - Verified existing `calc_ldl_martin_hopkins_extended(tc_mgdl, hdl_mgdl, tg_mgdl)` function in `ldlC/models.py`
  - Uses extended coefficient table with 10 TG ranges for finer granularity at high TG (400-800 mg/dL)
  - Extended table has same 30 non-HDL-C rows but 10 TG columns instead of 6
  - Factors decrease as TG increases (e.g., for high non-HDL: 11.9 → 6.0 from TG <100 to TG ≥800)
  - 11 comprehensive unit tests for high-TG cases already exist
- Files changed:
  - `PRD.md` (modified - marked US-012 complete)
- Learnings for future iterations:
  - Extended M-H differs from standard M-H primarily at TG > 400 where it has finer granularity
  - All 45 model tests now pass
---

## Iteration 12 - US-013: Create Equation Comparison Notebook
- What was implemented:
  - Created `notebooks/02_equation_comparison.ipynb` with comprehensive equation comparison
  - Generates synthetic grid of TC (150-300), HDL (40-70), TG (50-800) combinations
  - Creates heatmaps showing differences between equations across TG and TC ranges
  - Compares all four equations: Friedewald, Martin-Hopkins, Extended M-H, Sampson
  - Includes line plots showing LDL estimates as TG increases for fixed TC/HDL
  - Includes detailed markdown interpretation of when each equation excels
  - Generates visual recommendation matrix by TG stratum
- Files changed:
  - `notebooks/02_equation_comparison.ipynb` (new)
  - `PRD.md` (modified - marked US-013 complete)
- Learnings for future iterations:
  - Friedewald is unreliable for TG > 400, returns NaN
  - Martin-Hopkins and Sampson differ most at high TG (400-800 mg/dL)
  - Extended M-H provides finer granularity than standard M-H at high TG
  - All 66 tests pass
---

## Iteration 13 - US-014: Create Feature Engineering Module
- What was implemented:
  - Created `ldlC/train.py` module for ML training functions
  - Added `create_features(df)` function that generates feature matrix for ML training
  - Features include raw lipid values: tc_mgdl, hdl_mgdl, tg_mgdl, non_hdl_mgdl
  - Ratio features: tg_hdl_ratio, tc_hdl_ratio
  - Baseline equation predictions: ldl_friedewald, ldl_martin_hopkins, ldl_martin_hopkins_extended, ldl_sampson
  - Function returns (X, feature_names) tuple for use in training
  - Handles invalid inputs gracefully with NaN values
- Files changed:
  - `ldlC/train.py` (new)
  - `PRD.md` (modified - marked US-013 and US-014 complete)
- Learnings for future iterations:
  - Equation predictions are wrapped in try/except for robustness
  - Feature matrix uses pandas apply() for row-wise equation calculations
  - All 66 tests continue to pass
---

## Iteration 14 - US-015: Implement Train/Test Split with Stratification
- What was implemented:
  - Added `stratified_split(df, test_size=0.3, random_state=42)` function in `ldlC/train.py`
  - Stratifies by TG strata: < 100, 100-150, 150-200, 200-400, > 400 mg/dL
  - Uses sklearn's StratifiedShuffleSplit to ensure proportional representation
  - Returns (X_train, X_test, y_train, y_test) tuple
  - Validates required columns and minimum data size
  - Integrates with create_features() for feature generation
- Files changed:
  - `ldlC/train.py` (modified - added stratified_split function)
  - `PRD.md` (modified - marked US-015 complete)
- Learnings for future iterations:
  - TG strata based on clinical guidelines for hypertriglyceridemia
  - StratifiedShuffleSplit preserves class proportions in train/test splits
  - All 66 tests continue to pass
---

## Iteration 15 - US-017: Train Random Forest Model
- What was implemented:
  - Added `train_random_forest(X_train, y_train, n_estimators=200)` function in `ldlC/train.py`
  - Function trains a RandomForestRegressor with configurable n_estimators parameter
  - Uses random_state=42 for reproducibility and n_jobs=-1 for parallel processing
  - Validates input data shapes (X_train and y_train must have same length)
  - Returns fitted RandomForestRegressor model
- Files changed:
  - `ldlC/train.py` (modified - added RandomForestRegressor import and train_random_forest function)
  - `PRD.md` (modified - marked US-017 complete)
- Learnings for future iterations:
  - RandomForestRegressor from sklearn.ensemble for ensemble learning
  - Pattern follows same structure as train_ridge() for consistency
  - All 66 tests continue to pass
---

## Iteration 16 - US-018: Train LightGBM Model
- What was implemented:
  - Added `train_lightgbm(X_train, y_train, X_val, y_val)` function in `ldlC/train.py`
  - Uses LGBMRegressor from lightgbm package
  - Implements early stopping with 20 rounds (configurable)
  - Uses validation set for early stopping to prevent overfitting
  - Default n_estimators=1000 (early stopping typically terminates before this)
  - Uses random_state=42 for reproducibility and n_jobs=-1 for parallel processing
- Files changed:
  - `ldlC/train.py` (modified - added LGBMRegressor import and train_lightgbm function)
  - `PRD.md` (modified - marked US-018 complete)
- Learnings for future iterations:
  - LightGBM uses callbacks for early stopping (lightgbm.early_stopping())
  - verbosity=-1 suppresses training warnings in LightGBM
  - PowerShell doesn't support && for command chaining - use separate commands
  - All 66 tests continue to pass
---

## Iteration 17 - US-019: Implement Cross-Validation Wrapper
- What was implemented:
  - Added `cross_validate_model(model, X, y, n_splits=10)` function in `ldlC/train.py`
  - Uses sklearn's KFold for k-fold cross-validation with shuffling and random_state=42
  - Clones the model for each fold using sklearn.base.clone for independent training
  - Returns dict with RMSE_mean, RMSE_std, MAE_mean, MAE_std
  - Validates input data shapes and checks for minimum sample count
- Files changed:
  - `ldlC/train.py` (modified - added cross_validate_model function)
  - `PRD.md` (modified - marked US-019 complete)
- Learnings for future iterations:
  - Use sklearn.base.clone to get fresh model instance for each CV fold
  - KFold with shuffle=True and random_state=42 ensures reproducibility
  - All 66 tests continue to pass
---
