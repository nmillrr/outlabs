{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Evaluation and Validation\n",
                "\n",
                "This notebook provides comprehensive evaluation of trained free testosterone estimation models.\n",
                "\n",
                "**Contents:**\n",
                "1. Load trained models and test data\n",
                "2. Evaluate all models on internal test set\n",
                "3. Generate Bland-Altman plots for agreement analysis\n",
                "4. Subgroup analysis by SHBG tertile\n",
                "5. Bootstrap confidence intervals for key metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path for imports\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from joblib import load\n",
                "\n",
                "from freeT.evaluate import (\n",
                "    evaluate_model,\n",
                "    bland_altman_stats,\n",
                "    lins_ccc,\n",
                "    bootstrap_ci\n",
                ")\n",
                "from freeT.train import create_features, stratified_split\n",
                "from freeT.models import calc_ft_vermeulen\n",
                "\n",
                "print(\"Imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data and Models\n",
                "\n",
                "Load the processed NHANES data and trained models. If real data is unavailable, we generate synthetic data for demonstration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try to load real NHANES data, fallback to synthetic\n",
                "data_path = project_root / 'data' / 'processed' / 'nhanes_combined.csv'\n",
                "\n",
                "if data_path.exists():\n",
                "    print(\"Loading real NHANES data...\")\n",
                "    df = pd.read_csv(data_path)\n",
                "else:\n",
                "    print(\"Real data not found. Generating synthetic data for demonstration...\")\n",
                "    np.random.seed(42)\n",
                "    n_samples = 1000\n",
                "    \n",
                "    # Generate physiologically realistic values\n",
                "    df = pd.DataFrame({\n",
                "        'seqn': range(1, n_samples + 1),\n",
                "        'tt_nmoll': np.random.uniform(5, 35, n_samples),\n",
                "        'shbg_nmoll': np.random.uniform(10, 100, n_samples),\n",
                "        'alb_gl': np.random.uniform(35, 50, n_samples)\n",
                "    })\n",
                "    \n",
                "    # Calculate reference FT using Vermeulen\n",
                "    df['ft_reference'] = df.apply(\n",
                "        lambda row: calc_ft_vermeulen(\n",
                "            row['tt_nmoll'], row['shbg_nmoll'], row['alb_gl']\n",
                "        ),\n",
                "        axis=1\n",
                "    )\n",
                "    \n",
                "    # Add small noise to simulate realistic prediction targets\n",
                "    df['ft_reference'] = df['ft_reference'] + np.random.normal(0, 0.02, n_samples)\n",
                "    df['ft_reference'] = df['ft_reference'].clip(lower=0.01)\n",
                "\n",
                "print(f\"Data shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create features and split data\n",
                "X, feature_names = create_features(df)\n",
                "print(f\"Features: {feature_names}\")\n",
                "\n",
                "# Split with stratification by SHBG tertiles\n",
                "X_train, X_test, y_train, y_test = stratified_split(df, test_size=0.3, random_state=42)\n",
                "\n",
                "print(f\"Train size: {len(y_train)}, Test size: {len(y_test)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load trained models if available, otherwise train fresh\n",
                "models_dir = project_root / 'models'\n",
                "models = {}\n",
                "\n",
                "model_files = {\n",
                "    'Ridge': 'ridge_model.joblib',\n",
                "    'RandomForest': 'rf_model.joblib',\n",
                "    'LightGBM': 'lgbm_model.joblib',\n",
                "    'Best': 'best_model.joblib'\n",
                "}\n",
                "\n",
                "for name, filename in model_files.items():\n",
                "    model_path = models_dir / filename\n",
                "    if model_path.exists():\n",
                "        models[name] = load(model_path)\n",
                "        print(f\"Loaded {name} from {filename}\")\n",
                "\n",
                "# If no models found, train them on the fly\n",
                "if not models:\n",
                "    print(\"\\nNo saved models found. Training models for demonstration...\")\n",
                "    from freeT.train import train_ridge, train_random_forest, train_lightgbm\n",
                "    from sklearn.model_selection import train_test_split\n",
                "    \n",
                "    # Create validation set for LightGBM\n",
                "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
                "        X_train, y_train, test_size=0.2, random_state=42\n",
                "    )\n",
                "    \n",
                "    models['Ridge'] = train_ridge(X_train, y_train)\n",
                "    print(\"Trained Ridge\")\n",
                "    \n",
                "    models['RandomForest'] = train_random_forest(X_train, y_train, n_estimators=100)\n",
                "    print(\"Trained RandomForest\")\n",
                "    \n",
                "    models['LightGBM'] = train_lightgbm(X_tr, y_tr, X_val, y_val)\n",
                "    print(\"Trained LightGBM\")\n",
                "\n",
                "print(f\"\\nModels available: {list(models.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Evaluation on Test Set\n",
                "\n",
                "Evaluate all models using comprehensive metrics: RMSE, MAE, Bias, Pearson r, Lin's CCC, and Bland-Altman statistics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate all models\n",
                "results = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    if name == 'Best':\n",
                "        continue  # Skip duplicate best model\n",
                "    \n",
                "    y_pred = model.predict(X_test)\n",
                "    metrics = evaluate_model(y_test, y_pred, name)\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'RMSE': metrics['rmse'],\n",
                "        'MAE': metrics['mae'],\n",
                "        'Bias': metrics['bias'],\n",
                "        \"Pearson r\": metrics['r_pearson'],\n",
                "        \"Lin's CCC\": metrics['lin_ccc'],\n",
                "        'LOA Lower': metrics['ba_stats']['loa_lower'],\n",
                "        'LOA Upper': metrics['ba_stats']['loa_upper']\n",
                "    })\n",
                "\n",
                "# Also evaluate Vermeulen baseline (mechanistic solver)\n",
                "ft_vermeulen = X_test[:, feature_names.index('ft_vermeulen')]\n",
                "vermeulen_metrics = evaluate_model(y_test, ft_vermeulen, 'Vermeulen')\n",
                "\n",
                "results.append({\n",
                "    'Model': 'Vermeulen',\n",
                "    'RMSE': vermeulen_metrics['rmse'],\n",
                "    'MAE': vermeulen_metrics['mae'],\n",
                "    'Bias': vermeulen_metrics['bias'],\n",
                "    \"Pearson r\": vermeulen_metrics['r_pearson'],\n",
                "    \"Lin's CCC\": vermeulen_metrics['lin_ccc'],\n",
                "    'LOA Lower': vermeulen_metrics['ba_stats']['loa_lower'],\n",
                "    'LOA Upper': vermeulen_metrics['ba_stats']['loa_upper']\n",
                "})\n",
                "\n",
                "# Create comparison table\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df = results_df.round(4)\n",
                "print(\"\\n=== Model Comparison ===\")\n",
                "results_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Bland-Altman Plots\n",
                "\n",
                "Bland-Altman plots show the agreement between predicted and true values, with mean bias and 95% limits of agreement (LOA)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_bland_altman(y_true, y_pred, model_name, ax):\n",
                "    \"\"\"Generate Bland-Altman plot on given axes.\"\"\"\n",
                "    stats = bland_altman_stats(y_true, y_pred)\n",
                "    \n",
                "    mean_values = (y_true + y_pred) / 2\n",
                "    differences = y_pred - y_true\n",
                "    \n",
                "    ax.scatter(mean_values, differences, alpha=0.5, s=20)\n",
                "    ax.axhline(y=stats['mean_bias'], color='red', linestyle='-', \n",
                "               label=f\"Mean bias: {stats['mean_bias']:.4f}\")\n",
                "    ax.axhline(y=stats['loa_upper'], color='gray', linestyle='--',\n",
                "               label=f\"Upper LOA: {stats['loa_upper']:.4f}\")\n",
                "    ax.axhline(y=stats['loa_lower'], color='gray', linestyle='--',\n",
                "               label=f\"Lower LOA: {stats['loa_lower']:.4f}\")\n",
                "    ax.axhline(y=0, color='black', linestyle=':', alpha=0.5)\n",
                "    \n",
                "    ax.set_xlabel('Mean of True and Predicted (nmol/L)')\n",
                "    ax.set_ylabel('Difference (Predicted - True)')\n",
                "    ax.set_title(f'{model_name}')\n",
                "    ax.legend(fontsize=8, loc='best')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Create Bland-Altman plots for all models\n",
                "n_models = len([m for m in models.keys() if m != 'Best']) + 1  # +1 for Vermeulen\n",
                "fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 4))\n",
                "\n",
                "if n_models == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "plot_idx = 0\n",
                "for name, model in models.items():\n",
                "    if name == 'Best':\n",
                "        continue\n",
                "    y_pred = model.predict(X_test)\n",
                "    plot_bland_altman(y_test, y_pred, name, axes[plot_idx])\n",
                "    plot_idx += 1\n",
                "\n",
                "# Add Vermeulen baseline\n",
                "plot_bland_altman(y_test, ft_vermeulen, 'Vermeulen', axes[plot_idx])\n",
                "\n",
                "plt.suptitle('Bland-Altman Plots: Model Agreement Analysis', y=1.02, fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig(project_root / 'notebooks' / 'bland_altman_plots.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Predicted vs Actual Plots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create predicted vs actual plots\n",
                "fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 4))\n",
                "\n",
                "if n_models == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "plot_idx = 0\n",
                "for name, model in models.items():\n",
                "    if name == 'Best':\n",
                "        continue\n",
                "    y_pred = model.predict(X_test)\n",
                "    ccc = lins_ccc(y_test, y_pred)\n",
                "    \n",
                "    axes[plot_idx].scatter(y_test, y_pred, alpha=0.5, s=20)\n",
                "    \n",
                "    # Add identity line\n",
                "    min_val = min(y_test.min(), y_pred.min())\n",
                "    max_val = max(y_test.max(), y_pred.max())\n",
                "    axes[plot_idx].plot([min_val, max_val], [min_val, max_val], 'r--', label='Identity')\n",
                "    \n",
                "    axes[plot_idx].set_xlabel('True FT (nmol/L)')\n",
                "    axes[plot_idx].set_ylabel('Predicted FT (nmol/L)')\n",
                "    axes[plot_idx].set_title(f\"{name}\\nCCC = {ccc:.4f}\")\n",
                "    axes[plot_idx].legend()\n",
                "    axes[plot_idx].grid(True, alpha=0.3)\n",
                "    plot_idx += 1\n",
                "\n",
                "# Add Vermeulen baseline\n",
                "ccc = lins_ccc(y_test, ft_vermeulen)\n",
                "axes[plot_idx].scatter(y_test, ft_vermeulen, alpha=0.5, s=20)\n",
                "min_val = min(y_test.min(), ft_vermeulen.min())\n",
                "max_val = max(y_test.max(), ft_vermeulen.max())\n",
                "axes[plot_idx].plot([min_val, max_val], [min_val, max_val], 'r--', label='Identity')\n",
                "axes[plot_idx].set_xlabel('True FT (nmol/L)')\n",
                "axes[plot_idx].set_ylabel('Predicted FT (nmol/L)')\n",
                "axes[plot_idx].set_title(f\"Vermeulen\\nCCC = {ccc:.4f}\")\n",
                "axes[plot_idx].legend()\n",
                "axes[plot_idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('Predicted vs Actual Free Testosterone', y=1.02, fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig(project_root / 'notebooks' / 'predicted_vs_actual.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Subgroup Analysis by SHBG Tertile\n",
                "\n",
                "Analyze model performance across SHBG subgroups (Low, Medium, High) to identify potential systematic biases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create SHBG tertiles for test set\n",
                "shbg_test = X_test[:, feature_names.index('shbg_nmoll')]\n",
                "tertile_labels = pd.qcut(shbg_test, q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')\n",
                "\n",
                "print(\"SHBG tertile distribution in test set:\")\n",
                "print(pd.Series(tertile_labels).value_counts().sort_index())\n",
                "\n",
                "# Subgroup analysis for each model\n",
                "subgroup_results = []\n",
                "\n",
                "for tertile in ['Low', 'Medium', 'High']:\n",
                "    mask = tertile_labels == tertile\n",
                "    y_test_sub = y_test[mask]\n",
                "    X_test_sub = X_test[mask]\n",
                "    \n",
                "    for name, model in models.items():\n",
                "        if name == 'Best':\n",
                "            continue\n",
                "        \n",
                "        y_pred_sub = model.predict(X_test_sub)\n",
                "        metrics = evaluate_model(y_test_sub, y_pred_sub, name)\n",
                "        \n",
                "        subgroup_results.append({\n",
                "            'SHBG Tertile': tertile,\n",
                "            'Model': name,\n",
                "            'RMSE': metrics['rmse'],\n",
                "            'Bias': metrics['bias'],\n",
                "            \"Lin's CCC\": metrics['lin_ccc'],\n",
                "            'N': len(y_test_sub)\n",
                "        })\n",
                "    \n",
                "    # Vermeulen baseline\n",
                "    ft_verm_sub = X_test_sub[:, feature_names.index('ft_vermeulen')]\n",
                "    metrics = evaluate_model(y_test_sub, ft_verm_sub, 'Vermeulen')\n",
                "    subgroup_results.append({\n",
                "        'SHBG Tertile': tertile,\n",
                "        'Model': 'Vermeulen',\n",
                "        'RMSE': metrics['rmse'],\n",
                "        'Bias': metrics['bias'],\n",
                "        \"Lin's CCC\": metrics['lin_ccc'],\n",
                "        'N': len(y_test_sub)\n",
                "    })\n",
                "\n",
                "subgroup_df = pd.DataFrame(subgroup_results)\n",
                "print(\"\\n=== Subgroup Analysis by SHBG Tertile ===\")\n",
                "subgroup_df.round(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize subgroup performance\n",
                "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
                "\n",
                "metrics_to_plot = ['RMSE', 'Bias', \"Lin's CCC\"]\n",
                "tertiles = ['Low', 'Medium', 'High']\n",
                "model_names = [m for m in models.keys() if m != 'Best'] + ['Vermeulen']\n",
                "\n",
                "for idx, metric in enumerate(metrics_to_plot):\n",
                "    ax = axes[idx]\n",
                "    \n",
                "    x = np.arange(len(tertiles))\n",
                "    width = 0.2\n",
                "    \n",
                "    for i, model in enumerate(model_names):\n",
                "        values = [subgroup_df[(subgroup_df['Model'] == model) & \n",
                "                             (subgroup_df['SHBG Tertile'] == t)][metric].values[0] \n",
                "                  for t in tertiles]\n",
                "        ax.bar(x + i * width, values, width, label=model)\n",
                "    \n",
                "    ax.set_xlabel('SHBG Tertile')\n",
                "    ax.set_ylabel(metric)\n",
                "    ax.set_title(f'{metric} by SHBG Tertile')\n",
                "    ax.set_xticks(x + width * (len(model_names) - 1) / 2)\n",
                "    ax.set_xticklabels(tertiles)\n",
                "    ax.legend(fontsize=8)\n",
                "    ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.suptitle('Model Performance Across SHBG Subgroups', y=1.02, fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig(project_root / 'notebooks' / 'subgroup_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Bootstrap Confidence Intervals\n",
                "\n",
                "Calculate 95% confidence intervals for key metrics using bootstrap resampling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
                "\n",
                "def rmse_metric(y_true, y_pred):\n",
                "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "\n",
                "def mae_metric(y_true, y_pred):\n",
                "    return mean_absolute_error(y_true, y_pred)\n",
                "\n",
                "def bias_metric(y_true, y_pred):\n",
                "    return np.mean(y_pred - y_true)\n",
                "\n",
                "# Calculate bootstrap CIs for best performing model\n",
                "best_model_name = results_df.loc[results_df['RMSE'].idxmin(), 'Model']\n",
                "print(f\"Calculating bootstrap CIs for best model: {best_model_name}\")\n",
                "\n",
                "if best_model_name == 'Vermeulen':\n",
                "    y_pred_best = ft_vermeulen\n",
                "else:\n",
                "    y_pred_best = models[best_model_name].predict(X_test)\n",
                "\n",
                "ci_results = []\n",
                "\n",
                "for metric_name, metric_func in [('RMSE', rmse_metric), \n",
                "                                   ('MAE', mae_metric),\n",
                "                                   ('Bias', bias_metric),\n",
                "                                   (\"Lin's CCC\", lins_ccc)]:\n",
                "    print(f\"  Computing CI for {metric_name}...\")\n",
                "    lower, upper, mean = bootstrap_ci(\n",
                "        y_test, y_pred_best, metric_func, \n",
                "        n_bootstrap=2000, random_state=42\n",
                "    )\n",
                "    ci_results.append({\n",
                "        'Metric': metric_name,\n",
                "        'Point Estimate': mean,\n",
                "        '95% CI Lower': lower,\n",
                "        '95% CI Upper': upper\n",
                "    })\n",
                "\n",
                "ci_df = pd.DataFrame(ci_results).round(4)\n",
                "print(f\"\\n=== Bootstrap 95% CI for {best_model_name} ===\")\n",
                "ci_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary and Conclusions\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "This evaluation notebook demonstrates:\n",
                "\n",
                "1. **Model Comparison**: All models were evaluated on the test set using multiple metrics\n",
                "2. **Agreement Analysis**: Bland-Altman plots show the bias and limits of agreement\n",
                "3. **Subgroup Performance**: Models may perform differently across SHBG tertiles\n",
                "4. **Uncertainty Quantification**: Bootstrap CIs provide confidence bounds for metrics\n",
                "\n",
                "### Target Metrics (from PRD)\n",
                "- Mean bias < ±0.5 nmol/L\n",
                "- Lin's CCC ≥ 0.90"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary\n",
                "print(\"=\" * 60)\n",
                "print(\"FINAL MODEL EVALUATION SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "print(f\"\\nBest Model: {best_model_name}\")\n",
                "best_metrics = results_df[results_df['Model'] == best_model_name].iloc[0]\n",
                "\n",
                "print(f\"\\nPerformance Metrics:\")\n",
                "print(f\"  RMSE:      {best_metrics['RMSE']:.4f} nmol/L\")\n",
                "print(f\"  MAE:       {best_metrics['MAE']:.4f} nmol/L\")\n",
                "print(f\"  Bias:      {best_metrics['Bias']:.4f} nmol/L\")\n",
                "print(f\"  Pearson r: {best_metrics['Pearson r']:.4f}\")\n",
                "print(f\"  Lin's CCC: {best_metrics[\"Lin's CCC\"]:.4f}\")\n",
                "\n",
                "print(f\"\\nTarget Validation:\")\n",
                "bias_ok = abs(best_metrics['Bias']) < 0.5\n",
                "ccc_ok = best_metrics[\"Lin's CCC\"] >= 0.90\n",
                "\n",
                "print(f\"  Mean bias < ±0.5 nmol/L: {'✓ PASS' if bias_ok else '✗ FAIL'}\")\n",
                "print(f\"  Lin's CCC ≥ 0.90:        {'✓ PASS' if ccc_ok else '✗ FAIL'}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"Evaluation complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}